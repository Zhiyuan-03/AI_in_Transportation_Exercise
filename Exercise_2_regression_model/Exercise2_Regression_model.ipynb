{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jwxWYSMMweC"
      },
      "source": [
        "# **Regression**\n",
        "Imagine that you work for SL, and that one of your customers' biggest complaints is that they don't know what's the arrival delay for their buses. You decide to do something about it by building a model to predict the arrival delay at one bus stop. You intend to use the massive amounts of delay data the company has collected over the years to build a machine-learning model.\n",
        "\n",
        "Regression models predict numeric outcomes such as the price of a car, the age of a person, or the delay at a bus stop. Let's use a portion of a larger bus-delay dataset from Stockholm to train a regression model to predict the arrival delay given the dwell time, scheduled travel time, upstream stop delay, and recurrent delay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbJz5TA3JyBC"
      },
      "source": [
        "# **Tutorial 1: The development process of an AI model (linear regression)**\n",
        "In tutorial 1, we will give an example of a simple linear regression model to illustrate the process of developing an AI model which includes data exploration​, feature engineering, linear regression model specification​, model training​, and cross-validation.​\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGcXA3loalhv"
      },
      "source": [
        "## Load and prepare the data\n",
        "Start by loading the dataset and shaping it so that it's suitable for use in machine learning. This dataset is a subset of a much larger dataset. The data requires an amount of prep work before it's of any use at all. We here use a url link to load the data. If you cannot load the data via URL link, please download the data from GitHub and upload the data to colab manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAX-uM0-tltd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/zhenliangma/Applied-AI-in-Transportation/master/Exercise_2_regression_model/Exercise2BusData.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# df = pd.read_csv('Exercise2BusData.csv')\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0luS_xvkX_0"
      },
      "source": [
        "How many rows and columns does the dataset contain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oqOWyEvqF7B"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBtY9y0aqWnh"
      },
      "source": [
        "Are any of the columns missing values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZbnejxAwLBC"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBzn2CezpJiB"
      },
      "source": [
        "For simplicity, we only use 1000 records here. If your computer cannot run this tutorial in a short time, please set a smaller number in the following code. For example, 800."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjiQ_pe6pIcC"
      },
      "outputs": [],
      "source": [
        "df = df.iloc[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRttYQy-mF6C"
      },
      "source": [
        "Remove columns that are no longer needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-EmRGuNmDsz"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['Arrival_time','Stop_id','Bus_id','Line_id'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKhwhwhEkvSz"
      },
      "source": [
        "Draw a histogram by using seaborn to show the distribution of arrival delay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G0qaCW4wWHv"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "sns.histplot(x=df['Arrival_delay'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67PdCHUAmw2S"
      },
      "source": [
        "Find out how much influence input variables such as \"Upstream_stop_delay\" and \"Dwell_time\" have on the values in the \"Arrival_delay\" column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN3V8HK23meA"
      },
      "outputs": [],
      "source": [
        "corr_matrix = df.corr()\n",
        "corr_matrix['Arrival_delay'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO-fmrgXo6J5"
      },
      "source": [
        "Feature engineering visually explore and understand the relationships between different features (variables) in a dataset. Seaborn pair plots help identify patterns, correlations, and potential outliers, aiding in the selection and transformation of features to improve model performance. It is a crucial step in data preprocessing to enhance the accuracy and effectiveness of machine learning models.​"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIKKjOa56x9a"
      },
      "outputs": [],
      "source": [
        "x = df.drop(['Arrival_delay'], axis=1)  #drop the y; think about mse etc\n",
        "y = df['Arrival_delay']\n",
        "\n",
        "sns.pairplot(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JN6g5ehNM24"
      },
      "source": [
        "## Train / Test Split\n",
        "\n",
        "Before modeling we need to split our data into a training and test set. The training set is used to train the model and the test set to evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc7NUpVVNM25"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) #why set randomstate= 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlqm-Kptw7M3"
      },
      "source": [
        "## Create a linear regression model\n",
        "Now it's time build a regression model and train it with the data prepared in the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1nmDlwNoEiz"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression # easy model first\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)  # didnt specify parameter tuning; used default\n",
        "\n",
        "# Predict the test data with the fitted model\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#xgboost\n",
        "import xgboost as xgb\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "params = {\n",
        "    \"booster\": \"gblinear\",         # Use linear base learner\n",
        "    \"objective\": \"reg:squarederror\"  # Regression objective\n",
        "}\n",
        "xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=100)\n",
        "preds = xgb_model.predict(dtest)\n",
        "\n"
      ],
      "metadata": {
        "id": "Gtd2F5HcdWOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCgwHsqGvXKo"
      },
      "source": [
        "## Evaluate the model\n",
        "You can evaluate the model's performance using various metrics, such as Mean Absolute Error (MAE), Mean Squared Error (MSE), or R-squared (R2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqPg05l1vf59"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW47guJtvqAW"
      },
      "source": [
        "## Visualize the Results\n",
        "we create a \"Actual vs. Predicted Values\" graph to give a visual inspection of the prediction quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85GnzAS6oNyW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# y_test contains the actual target values for the test dataset\n",
        "# y_pred contains the predicted values for the test dataset\n",
        "\n",
        "# Create a scatter plot to visualize the relationship\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)  # Plot actual vs. predicted values\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted Values\")\n",
        "\n",
        "# Add a diagonal line for reference (perfect predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', lw=2)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2s8moatbDE0"
      },
      "source": [
        "# **Tutorial 2: AI model training techniques (SVM)**\n",
        "We will give a SVM model to illustrate how to use training techniques including train-test split, normalization, and grid search to find the best hyperparameters for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF_UVjsjbm1G"
      },
      "source": [
        "## Train-test split\n",
        "The purpose of a \"train-test split\" is to evaluate the performance of a machine learning model. It involves dividing a dataset into two subsets: the training set, used to train the model, and the test set, used to assess how well the model generalizes to new, unseen data. This process helps identify if the model is overfitting (performing well on the training data but poorly on new data) or underfitting (performing poorly on both training and test data), allowing for adjustments to improve model accuracy and generalization.\n",
        "\n",
        "We have already split the data in Tutorial 1. Therefore, we just put the code here and comment it out to just remind you of the importance of train-test split in modeling training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypA1w6u0Xm9m"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tun6bJlXcMAe"
      },
      "source": [
        "## Normalization\n",
        "Normalization is essential, especially when using SVMs, as it's sensitive to the scale of features. We can use StandardScaler from scikit-learn to normalize the data. It transforms the data so that each feature has a mean of 0 and a standard deviation of 1, which can be important for certain algorithms that are sensitive to the scale of input features, like gradient descent-based optimization methods. Other scalers include Min-Max scaling (\"MinMaxScaler\"), which scales features to a specific range (typically [0, 1]), and Robust scaling (\"RobustScaler\"), which is less affected by outliers in the data compared to StandardScaler. These scalers are used to preprocess data and make it more suitable for machine learning algorithms.\n",
        "\n",
        "It is important to fit the scaler on the training data only and afterwards apply the fitted scaler to the test data. When fitting the scaler on both the train and test data there might be unwanted information leackage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjH7sVaTvlOu"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVPmihFccQgd"
      },
      "source": [
        "## Grid search\n",
        "Hyperparameter tuning helps find the best set of hyperparameters for your model. We'll use GridSearchCV from scikit-learn to search for the best combination of parameters. Grid search is a hyperparameter tuning technique in machine learning used to systematically search through a predefined set of hyperparameter combinations for a given model to identify the best-performing configuration. It automates the process of selecting the optimal hyperparameters, helping improve the model's performance and generalization on unseen data by finding the best parameter values that minimize errors or maximize a chosen performance metric.\n",
        "\n",
        "The GridSearchCV applies for each parameter set a cross validation (in this case five splits). By executing this cross validation we can increase the robustness of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdiPznRVwHai"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Define a parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'C': [ 1, 10],\n",
        "    'epsilon': [ 1, 10]\n",
        "}   # specify the settings, try out all combinations of parameters output the best parameter to retrain the model; change the setting in exercise\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(SVR(), param_grid, cv=5, verbose=2)\n",
        "\n",
        "# Fit the grid search to the scaled training data\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmmWdaf7cgEG"
      },
      "source": [
        "## Train a SVM regression model\n",
        "Now, create an SVM regression model using the best parameters and train it using the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiJ8sWm89Dww"
      },
      "outputs": [],
      "source": [
        "# Create an SVR model with the best parameters from the grid search\n",
        "best_svr = SVR(kernel=best_params['kernel'], C=best_params['C'], epsilon=best_params['epsilon'])\n",
        "best_svr.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-N7DbrldmTN"
      },
      "source": [
        "## Make predictions\n",
        "Use the trained model to make predictions on the test data and evaluate the performance of the SVM regression model using metrics like Mean Squared Error (MSE) and R-squared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXBMN7IdOaj_"
      },
      "outputs": [],
      "source": [
        "y_pred = best_svr.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BREK7ODZoYdY"
      },
      "source": [
        "## Let's see the predictions\n",
        "we create a \"Actual vs. Predicted Values\" graph again to give a visual inspection of the prediction quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuquJAVtnRIQ"
      },
      "outputs": [],
      "source": [
        "# y_test contains the actual target values for the test dataset\n",
        "# y_pred contains the predicted values for the test dataset\n",
        "\n",
        "# Create a scatter plot to visualize the relationship\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)  # Plot actual vs. predicted values\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted Values\")\n",
        "\n",
        "# Add a diagonal line for reference (perfect predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', lw=2)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}